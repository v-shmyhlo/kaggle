{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "print(os.listdir(\"../input/imet-2019-fgvc6\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import cv2\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from PIL import Image\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "DATASET_PATH = '../input/imet-2019-fgvc6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
    "train_data['attribute_ids'] = train_data['attribute_ids'].apply(lambda s: [int(x) for x in s.split()])\n",
    "\n",
    "classes = pd.read_csv(os.path.join(DATASET_PATH, 'labels.csv'))\n",
    "id_to_class = {row['attribute_id']: row['attribute_name'] for _, row in classes.iterrows()}\n",
    "class_to_id = {id_to_class[k]: k for k in id_to_class}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_shape(id):\n",
    "#     return cv2.imread(os.path.join('../input/train/{}.png'.format(id))).shape\n",
    "\n",
    "# with Pool(os.cpu_count()) as pool:\n",
    "#     sizes = pool.map(load_shape, tqdm(train_data['id']))\n",
    "\n",
    "# del x\n",
    "# id = train_data.id.values[0]\n",
    "# im = \n",
    "# print(im.shape)\n",
    "\n",
    "# _, x = cv2.imencode('.jpg', im)\n",
    "# print(im.size)\n",
    "# print(x.shape)\n",
    "\n",
    "# def tmp(id):\n",
    "#     image = cv2.imread(os.path.join(DATASET_PATH, 'train/{}.png'.format(id)))\n",
    "#     image = cv2.resize(image, (128, 128))\n",
    "#     image = cv2.imencode('.jpg', image)\n",
    "\n",
    "# x = [tmp(id) for id in tqdm(train_data.id)]\n",
    "# x = [Image.open(os.path.join(DATASET_PATH, 'train/{}.png'.format(id))) for id in tqdm(train_data.id)]\n",
    "# print(len(x), x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['height'] = [size[0] for size in sizes]\n",
    "# train_data['width'] = [size[1] for size in sizes]\n",
    "# train_data['ratio'] = train_data['width'] / train_data['height']\n",
    "\n",
    "# # plt.scatter(train_data['width'], train_data['height'])\n",
    "# print(train_data[['width', 'height', 'ratio']].describe())\n",
    "\n",
    "# print('min', train_data['ratio'].idxmin())\n",
    "# print('max', train_data['ratio'].idxmax())\n",
    "# print(train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(200,200))\n",
    "# plt.imshow(cv2.imread('../input/train/{}.png'.format(train_data.loc[47121]['id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls = train_data['attribute_ids'].apply(lambda xs: [id_to_class[x] for x in xs])\n",
    "\n",
    "\n",
    "# print(labels.head())\n",
    "# # ls = labels.apply(lambda xs: [x for x in xs if x.startswith('culture')])\n",
    "\n",
    "# print(ls.head())\n",
    "\n",
    "# ls = ls.apply(len)\n",
    "# print(len(ls))\n",
    "# print(ls.hist(bins=100))\n",
    "\n",
    "# print(ls.value_counts())\n",
    "\n",
    "# # print(labels[(ls == 4).values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEvalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        row = self.data.iloc[i]\n",
    "        \n",
    "        image = load_image(os.path.join(DATASET_PATH, 'train/{}.png'.format(row['id'])))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "        for l in row['attribute_ids']: # TODO:\n",
    "            label[l] = 1.\n",
    "            \n",
    "        return image, label, row['id']   \n",
    "    \n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.data = os.listdir(os.path.join(DATASET_PATH, 'test'))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        path = self.data[i]\n",
    "        id = os.path.splitext(path)[0]\n",
    "        \n",
    "        image = load_image(os.path.join(DATASET_PATH, 'test', path))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, id\n",
    "    \n",
    "# def load_image(path, size):\n",
    "#     image = cv2.imread(path)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image = cv2.resize(image, size)\n",
    "    \n",
    "#     return image\n",
    "\n",
    "# def load_image(path, size):\n",
    "#     image = Image.open(path)\n",
    "#     image = image.resize(size)\n",
    "#     image = np.array(image)\n",
    "    \n",
    "#     return image\n",
    "\n",
    "def load_image(path):\n",
    "    image = Image.open(path)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        if ARCH == 'resnet34':\n",
    "            block = torchvision.models.resnet.BasicBlock\n",
    "            self.model = ResNet(block, [3, 4, 6, 3])\n",
    "            self.model.load_state_dict(torch.load('../input/resnet34/resnet34.pth'))\n",
    "            self.model.fc = nn.Linear(512 * block.expansion, NUM_CLASSES)\n",
    "        elif ARCH == 'resnet50':\n",
    "            block = torchvision.models.resnet.Bottleneck\n",
    "            self.model = ResNet(block, [3, 4, 6, 3])\n",
    "            self.model.load_state_dict(torch.load('../input/resnet50/resnet50.pth'))\n",
    "            self.model.fc = nn.Linear(512 * block.expansion, NUM_CLASSES)\n",
    "        elif ARCH == 'seresnext50':\n",
    "            block = SEResNeXtBottleneck\n",
    "            self.model = SENet(\n",
    "                block, [3, 4, 6, 3], groups=32, reduction=16, dropout_p=None, \n",
    "                inplanes=64, input_3x3=False, downsample_kernel_size=1, downsample_padding=0,\n",
    "                num_classes=1000)\n",
    "            self.model.load_state_dict(torch.load('../input/torch-models/se_resnext50_32x4d.pth'))\n",
    "            self.model.last_linear = nn.Linear(512 * block.expansion, NUM_CLASSES)\n",
    "        else:\n",
    "            raise AssertionError('invalid ARCH {}'.format(ARCH))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.weight = nn.Conv2d(in_features, 1, kernel_size=1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        b, c, h, w = input.size()\n",
    "        \n",
    "        weight = self.weight(input)\n",
    "        weight = weight.view(b, 1, h * w)\n",
    "        weight = weight.softmax(-1)\n",
    "\n",
    "        input = input.view(b, c, h * w)\n",
    "        input = input * weight\n",
    "        input = input.sum(-1)\n",
    "        input = input.view(b, c, 1, 1)\n",
    "        \n",
    "        return input\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class SENet(nn.Module):\n",
    "    def __init__(self, block, layers, groups, reduction, dropout_p=0.2, inplanes=128, input_3x3=True, \n",
    "                 downsample_kernel_size=3, downsample_padding=1, num_classes=1000):\n",
    "        super(SENet, self).__init__()\n",
    "        \n",
    "        self.inplanes = inplanes\n",
    "        if input_3x3:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1, bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(64)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=False)),\n",
    "                ('bn2', nn.BatchNorm2d(64)),\n",
    "                ('relu2', nn.ReLU(inplace=True)),\n",
    "                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1, bias=False)),\n",
    "                ('bn3', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu3', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        else:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        # To preserve compatibility with Caffe weights `ceil_mode=True`\n",
    "        # is used instead of `padding=1`.\n",
    "        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2, ceil_mode=True)))\n",
    "        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n",
    "        self.layer1 = self._make_layer(\n",
    "            block,\n",
    "            planes=64,\n",
    "            blocks=layers[0],\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=1,\n",
    "            downsample_padding=0)\n",
    "        self.layer2 = self._make_layer(\n",
    "            block,\n",
    "            planes=128,\n",
    "            blocks=layers[1],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding)\n",
    "        self.layer3 = self._make_layer(\n",
    "            block,\n",
    "            planes=256,\n",
    "            blocks=layers[2],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding)\n",
    "        self.layer4 = self._make_layer(\n",
    "            block,\n",
    "            planes=512,\n",
    "            blocks=layers[3],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n",
    "        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n",
    "                    downsample_kernel_size=1, downsample_padding=0):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.inplanes, planes * block.expansion, kernel_size=downsample_kernel_size, \n",
    "                    stride=stride, padding=downsample_padding, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, groups, reduction, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups, reduction))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out = self.se_module(out) + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class SEResNeXtBottleneck(Bottleneck):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None, base_width=4):\n",
    "        super(SEResNeXtBottleneck, self).__init__()\n",
    "        \n",
    "        width = math.floor(planes * (base_width / 64)) * groups\n",
    "        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return module_input * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(object):\n",
    "    def __init__(self, optimizer, lr, beta, max_steps, annealing):\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.max_steps = max_steps\n",
    "        self.annealing = annealing\n",
    "        self.epoch = -1\n",
    "    \n",
    "    def step(self):\n",
    "        self.epoch += 1\n",
    "        \n",
    "        mid = round(self.max_steps * 0.3)\n",
    "        if self.epoch < mid:\n",
    "            r = self.epoch / mid\n",
    "            lr = self.annealing(self.lr[0], self.lr[1], r)\n",
    "            beta = self.annealing(self.beta[0], self.beta[1], r)\n",
    "        else:\n",
    "            r = (self.epoch - mid) / (self.max_steps - mid)\n",
    "            lr = self.annealing(self.lr[1], self.lr[0] / 1e4, r)\n",
    "            beta = self.annealing(self.beta[1], self.beta[0], r)\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "            if 'betas' in param_group:\n",
    "                param_group['betas'] = (beta, *param_group['betas'][1:])\n",
    "            elif 'momentum' in param_group:\n",
    "                param_group['momentum'] = beta\n",
    "            else:\n",
    "                raise AssertionError('no beta parameter')\n",
    "\n",
    "def annealing_linear(start, end, r):\n",
    "    return start + r * (end - start)            \n",
    "            \n",
    "def annealing_cos(start, end, r):\n",
    "    cos_out = np.cos(np.pi * r) + 1\n",
    "    return end + (start - end) / 2 * cos_out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mean(object):\n",
    "    def __init__(self):\n",
    "        self.values = []\n",
    "\n",
    "    def compute(self):\n",
    "        return sum(self.values) / len(self.values)\n",
    "\n",
    "    def update(self, value):\n",
    "        self.values.extend(np.reshape(value, [-1]))\n",
    "\n",
    "    def reset(self):\n",
    "        self.values = []\n",
    "\n",
    "    def compute_and_reset(self):\n",
    "        value = self.compute()\n",
    "        self.reset()\n",
    "        \n",
    "        return value\n",
    "    \n",
    "class EWA(object):\n",
    "    def __init__(self, beta=0.9):\n",
    "        self.beta = beta\n",
    "        self.step = 0\n",
    "        self.average = 0\n",
    "        \n",
    "    def update(self, value):\n",
    "        self.step += 1\n",
    "        self.average = self.beta * self.average + (1 - self.beta) * value\n",
    "        \n",
    "    def compute(self):\n",
    "        return self.average / (1 - self.beta ** self.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = target.float()\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "               ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        if len(loss.size())==2:\n",
    "            loss = loss.sum(dim=1)\n",
    "        return loss.mean()\n",
    "\n",
    "def f2_loss(input, target, eps=1e-7):\n",
    "    input = input.sigmoid()\n",
    "\n",
    "    tp = (target * input).sum(1)\n",
    "    tn = ((1 - target) * (1 - input)).sum(1)\n",
    "    fp = ((1 - target) * input).sum(1)\n",
    "    fn = (target * (1 - input)).sum(1)\n",
    "\n",
    "    p = tp / (tp + fp + eps)\n",
    "    r = tp / (tp + fn + eps)\n",
    "    \n",
    "    beta_sq = 2**2\n",
    "    f2 = (1 + beta_sq) * p * r / (beta_sq * p + r + eps)\n",
    "    loss = -(f2 + eps).log()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def hinge_loss(input, target, delta=1.):\n",
    "    target = target * 1. + (1 - target) * -1.\n",
    "    loss = torch.max(torch.zeros_like(input).to(input.device), delta - target * input)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compute_loss(input, target):\n",
    "    loss = [l(input=input, target=target).mean() for l in LOSS]\n",
    "    loss = sum(loss) / len(loss)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compute_score(input, target, threshold=0.5):\n",
    "    input = (input.sigmoid() > threshold).float()\n",
    "\n",
    "    tp = (target * input).sum(-1)\n",
    "    tn = ((1 - target) * (1 - input)).sum(-1)\n",
    "    fp = ((1 - target) * input).sum(-1)\n",
    "    fn = (target * (1 - input)).sum(-1)\n",
    "\n",
    "    p = tp / (tp + fp)\n",
    "    r = tp / (tp + fn)\n",
    "    \n",
    "    beta_sq = 2**2\n",
    "    f2 = (1 + beta_sq) * p * r / (beta_sq * p + r)\n",
    "    f2[f2 != f2] = 0.\n",
    "    \n",
    "    return f2\n",
    "\n",
    "def find_threshold_global(input, target):\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    scores = [compute_score(input=input, target=target, threshold=t).mean() \n",
    "              for t in tqdm(thresholds, desc='threshold search')]\n",
    "    threshold = thresholds[np.argmax(scores)]\n",
    "    score = scores[np.argmax(scores)]\n",
    "    \n",
    "    plt.plot(thresholds, scores)\n",
    "    plt.axvline(threshold)\n",
    "    plt.title('score: {:.8f}, threshold: {:.8f}'.format(score.item(), threshold))\n",
    "    plt.show()\n",
    "    \n",
    "    return threshold\n",
    "\n",
    "# def find_threshold_class(input, target, initial):\n",
    "#     threshold = torch.full((NUM_CLASSES, ), initial).to(input.device)\n",
    "    \n",
    "#     tmp = []\n",
    "#     for _ in range(5):\n",
    "#         for i in tqdm(range(NUM_CLASSES), desc='threshold search'):\n",
    "#             space = np.arange(0.1, 0.9, 0.01)\n",
    "#             scores = []\n",
    "\n",
    "#             for t in space:\n",
    "#                 threshold[i] = t\n",
    "#                 score = compute_score(input=input, target=target, threshold=threshold).mean() \n",
    "#                 scores.append(score)\n",
    "\n",
    "#             threshold[i] = space[np.argmax(scores)]\n",
    "#             score = scores[np.argmax(scores)]\n",
    "#         tmp.append(score)\n",
    "#         print(score)\n",
    "#     print(tmp)\n",
    "    \n",
    "#     plt.hist(threshold.cpu(), bins=50)\n",
    "#     plt.title('score: {:.8f}, threshold mean: {:.8f}, threshold std: {:.8f}'.format(score.item(), threshold.mean(), threshold.std()))\n",
    "#     plt.show()\n",
    "    \n",
    "#     return threshold\n",
    "\n",
    "def find_threshold_class(input, target, initial):\n",
    "    threshold = torch.full((NUM_CLASSES, ), initial).to(input.device)\n",
    "    steps = []\n",
    "    for _ in tqdm(range(50), desc='threshold search'):\n",
    "        for i in np.random.permutation(NUM_CLASSES):\n",
    "            r = torch.tensor([threshold[i] - 0.01, threshold[i], threshold[i] + 0.01])\n",
    "            space = threshold.view(1, NUM_CLASSES).repeat(r.size(0), 1)\n",
    "            space[:, i] = r\n",
    "            scores = compute_score(input=input.unsqueeze(0), target=target.unsqueeze(0), threshold=space.unsqueeze(1))\n",
    "            scores = scores.mean(-1)\n",
    "            threshold[i] = r[scores.argmax()]\n",
    "            \n",
    "        score = compute_score(input=input, target=target, threshold=threshold).mean()\n",
    "        steps.append(score)\n",
    "        \n",
    "    plt.plot(steps)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(threshold.cpu(), bins=50)\n",
    "    plt.title('score: {:.8f}, threshold mean: {:.8f}, threshold std: {:.8f}'.format(score.item(), threshold.mean(), threshold.std()))\n",
    "    plt.show()\n",
    "    \n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "NUM_CLASSES = len(classes)\n",
    "BATCH_SIZE = 256\n",
    "WORKERS = 0\n",
    "ANNEAL = annealing_linear\n",
    "ARCH = 'seresnext50'\n",
    "OPT = 'adam'\n",
    "AUG = 'med'\n",
    "IMAGE_SIZE = 128\n",
    "LOSS_SMOOTHING = 0.9\n",
    "LOSS = [\n",
    "#     f2_loss,\n",
    "#     F.binary_cross_entropy_with_logits,\n",
    "    FocalLoss(),\n",
    "#     hinge_loss,\n",
    "]\n",
    "\n",
    "# TODO: pin memory\n",
    "# TODO: stochastic weight averaging\n",
    "# TODO: group images by buckets (size, ratio) and batch\n",
    "# TODO: hinge loss clamp instead of minimum\n",
    "# TODO: losses\n",
    "# TODO: better one cycle\n",
    "# TODO: cos vs lin\n",
    "# TODO: load and restore state after lr finder\n",
    "# TODO: better loss smoothing\n",
    "# TODO: shuffle thresh search\n",
    "# TODO: init thresh search from global best\n",
    "# TODO: shuffle split\n",
    "# TODO: tune on large size\n",
    "# TODO: cross val\n",
    "# TODO: smart sampling\n",
    "# TODO: larger model\n",
    "# TODO: imagenet papers\n",
    "# TODO: load image as jpeg\n",
    "# TODO: augmentations (flip, crops, color)\n",
    "# TODO: min 1 tag?\n",
    "# TODO: pick threshold to match ratio\n",
    "# TODO: compute smoothing beta from batch size and num steps\n",
    "# TODO: speedup image loading\n",
    "# TODO: pin memory\n",
    "# TODO: smart sampling\n",
    "# TODO: better threshold search (step, epochs)\n",
    "# TODO: weight standartization\n",
    "# TODO: label smoothing\n",
    "# TODO: search threshold for each class\n",
    "# TODO: build sched for lr find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor_and_norm = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "if AUG == 'low':\n",
    "    train_transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        to_tensor_and_norm,\n",
    "    ])\n",
    "    eval_transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        to_tensor_and_norm,\n",
    "    ])\n",
    "elif AUG == 'med':\n",
    "    train_transform = T.Compose([\n",
    "        T.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=(1., 1.), ratio=(3. / 4., 4. / 3.)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        to_tensor_and_norm,\n",
    "    ])\n",
    "    eval_transform = T.Compose([\n",
    "        T.Resize(IMAGE_SIZE),\n",
    "        T.CenterCrop(IMAGE_SIZE),\n",
    "        to_tensor_and_norm,\n",
    "    ])\n",
    "elif AUG == 'med+color':\n",
    "    train_transform = T.Compose([\n",
    "        T.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=(1., 1.), ratio=(3. / 4., 4. / 3.)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        to_tensor_and_norm,\n",
    "    ])\n",
    "    eval_transform = T.Compose([\n",
    "        T.Resize(IMAGE_SIZE),\n",
    "        T.CenterCrop(IMAGE_SIZE),\n",
    "        to_tensor_and_norm,\n",
    "    ])\n",
    "elif AUG == 'hard':\n",
    "    scale = (0.6, 1.0)\n",
    "    image_size_corrected = round(IMAGE_SIZE * (1 / np.mean(scale).item()))\n",
    "        \n",
    "    train_transform = T.Compose([\n",
    "        T.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=scale, ratio=(3. / 4., 4. / 3.)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        to_tensor_and_norm,\n",
    "    ])\n",
    "    eval_transform = T.Compose([\n",
    "        T.Resize(image_size_corrected),\n",
    "        T.CenterCrop(image_size_corrected),\n",
    "        to_tensor_and_norm,\n",
    "    ])\n",
    "else:\n",
    "    raise AssertionError('invalid AUG {}'.format(AUG))\n",
    "\n",
    "s = len(train_data)\n",
    "\n",
    "train_dataset = TrainEvalDataset(train_data.iloc[s//5:], transform=train_transform)\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, drop_last=True, shuffle=True, num_workers=WORKERS) # TODO: all args\n",
    "\n",
    "eval_dataset = TrainEvalDataset(train_data.iloc[:s//5], transform=eval_transform)\n",
    "eval_data_loader = torch.utils.data.DataLoader(\n",
    "    eval_dataset, batch_size=BATCH_SIZE, num_workers=WORKERS) # TODO: all args\n",
    "\n",
    "test_dataset = TestDataset(transform=eval_transform)\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, num_workers=WORKERS) # TODO: all args\n",
    "\n",
    "### <<<\n",
    "def build_optimizer(parameters, lr, weight_decay):\n",
    "    if OPT == 'adam':\n",
    "        return torch.optim.Adam(parameters, lr, weight_decay=weight_decay)\n",
    "    elif OPT == 'sgd':\n",
    "        return torch.optim.SGD(parameters, lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise AssertionError('invalid OPT {}'.format(OPT))\n",
    "\n",
    "def set_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "min_lr = 1e-8\n",
    "max_lr = 10.\n",
    "p = (max_lr / min_lr)**(1 / len(train_data_loader))\n",
    "\n",
    "stp = 0\n",
    "cur_lr = min_lr\n",
    "lrs = []\n",
    "lss = []\n",
    "sls = []\n",
    "ewa = EWA(beta=LOSS_SMOOTHING)\n",
    "\n",
    "minima = {\n",
    "    'loss': np.inf,\n",
    "    'lr': min_lr\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "optimizer = build_optimizer(model.parameters(), min_lr, weight_decay=1e-4)\n",
    "\n",
    "model.train()\n",
    "for images, labels, ids in tqdm(train_data_loader, desc='lr search'):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    logits = model(images)\n",
    "\n",
    "    loss = compute_loss(input=logits, target=labels)\n",
    "\n",
    "    ### <<<\n",
    "    ewa.update(loss.data.cpu().numpy().mean())\n",
    "    lrs.append(cur_lr)\n",
    "    lss.append(loss.data.cpu().numpy().mean())\n",
    "    sls.append(ewa.compute())\n",
    "    if ewa.compute() < minima['loss']:\n",
    "        minima['loss'] = ewa.compute()\n",
    "        minima['lr'] = cur_lr\n",
    "    if minima['loss'] * 4 < ewa.compute():\n",
    "        break\n",
    "    stp += 1\n",
    "\n",
    "    cur_lr = min_lr * p**stp\n",
    "    set_lr(optimizer, cur_lr)\n",
    "    ### <<<\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#     break # <<<<<<<<<<<<<<\n",
    "\n",
    "plt.plot(lrs, lss)\n",
    "plt.plot(lrs, sls)\n",
    "plt.axvline(minima['lr'])\n",
    "plt.xscale('log')\n",
    "plt.title('loss: {:.8f}, lr: {:.8f}'.format(minima['loss'], minima['lr']))\n",
    "plt.show()\n",
    "### <<<\n",
    "\n",
    "del model\n",
    "del optimizer\n",
    "del images, labels, logits, loss\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "optimizer = build_optimizer(model.parameters(), minima['lr'] / 10, weight_decay=1e-4)\n",
    "scheduler = OneCycleScheduler(\n",
    "    optimizer, lr=(minima['lr'] / 10 / 25, minima['lr'] / 10), beta=(0.95, 0.85), max_steps=len(train_data_loader) * EPOCHS, annealing=ANNEAL)\n",
    "\n",
    "metrics = {\n",
    "    'loss': Mean(),\n",
    "    'score': Mean()\n",
    "}\n",
    "\n",
    "stats = {\n",
    "    'train_loss': [],\n",
    "    'eval_loss': [],\n",
    "    'eval_score': []\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for images, labels, ids in tqdm(train_data_loader, desc='epoch {} train'.format(epoch)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "\n",
    "        loss = compute_loss(input=logits, target=labels)\n",
    "        metrics['loss'].update(loss.data.cpu().numpy())\n",
    "        \n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         break # <<<<<<<<<\n",
    "        \n",
    "    loss = metrics['loss'].compute_and_reset()\n",
    "    stats['train_loss'].append(loss)\n",
    "    print('[TRAIN] loss: {:.8f}'.format(loss))    \n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, ids in tqdm(eval_data_loader, desc='epoch {} evaluation'.format(epoch)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "\n",
    "            loss = compute_loss(input=logits, target=labels)\n",
    "            metrics['loss'].update(loss.data.cpu().numpy())\n",
    "            score = compute_score(input=logits, target=labels)\n",
    "            metrics['score'].update(score.data.cpu().numpy())\n",
    "            \n",
    "#             break # <<<<<<<<<\n",
    "            \n",
    "    loss = metrics['loss'].compute_and_reset()\n",
    "    stats['eval_loss'].append(loss)\n",
    "    score = metrics['score'].compute_and_reset()\n",
    "    stats['eval_score'].append(score)\n",
    "    print('[EVAL] loss: {:.8f}, score: {:.8f}'.format(loss, score))\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        torch.save(model.state_dict(), './model.pth')\n",
    "        \n",
    "#     break # <<<<<<<<<<\n",
    "\n",
    "plt.plot(stats['train_loss'])\n",
    "plt.plot(stats['eval_loss'])\n",
    "plt.show()\n",
    "plt.plot(stats['eval_score'])\n",
    "plt.show()\n",
    "        \n",
    "model.load_state_dict(torch.load('./model.pth'))\n",
    "predictions = []\n",
    "targets = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, ids in tqdm(eval_data_loader, desc='best model evaluation'.format(epoch)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "        \n",
    "        targets.append(labels)\n",
    "        predictions.append(logits)\n",
    "        \n",
    "#         break # <<<<<<<<\n",
    "\n",
    "    del images, labels, logits\n",
    "\n",
    "    predictions = torch.cat(predictions, 0)\n",
    "    targets = torch.cat(targets, 0)\n",
    "    threshold = find_threshold_global(input=predictions, target=targets)\n",
    "#     threshold = find_threshold_class(input=predictions, target=targets, initial=threshold)\n",
    "\n",
    "submission = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, ids in tqdm(test_data_loader, desc='inference'):\n",
    "        images = images.to(device)\n",
    "        logits = model(images)\n",
    "\n",
    "        for id, logits in zip(ids, logits):\n",
    "            pred = (logits.sigmoid() > threshold).nonzero().reshape(-1)\n",
    "            pred = pred.data.cpu().numpy()\n",
    "            pred = map(str, pred)\n",
    "            pred = ' '.join(pred)\n",
    "\n",
    "            submission.append((id, pred))\n",
    "            \n",
    "submission = pd.DataFrame(submission, columns=['id', 'attribute_ids'])\n",
    "submission.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
